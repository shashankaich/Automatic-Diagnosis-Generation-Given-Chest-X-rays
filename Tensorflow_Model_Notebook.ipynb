{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_Model_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BcweE2DgaFCE"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urm3q6WJB8sb"
      },
      "source": [
        "# Automatic Diagnosis Generation Given Chest X-rays With Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VODkw6TuxU4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4415dae-0736-4175-d58d-550607003fd4"
      },
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "drive_path = '/content/gdrive/My Drive/Assignments_Drive/Case_Study_2/Medical_Data'\n",
        "# specifying paths\n",
        "txt_path = drive_path + '/ecgen'\n",
        "img_path = drive_path + '/images'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO11sbfWCbhH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6cbefa80-1fad-4cf0-ad72-ec53eb37476f"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('IPython.notebook.execute_cells_below()'))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.execute_cells_below()"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YWuq8qBN8aG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c08a18-421e-4363-f4c8-9ede2b6e2be2"
      },
      "source": [
        "# imports\n",
        "#!pip install tensorflow-gpu==2.3\n",
        "#!pip install scikit-learn==0.20.4\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.txt.gz\"\n",
        "\n",
        "# from tensorflow.python.framework import ops\n",
        "# ops.disable_eager_execution()\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "import io\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from PIL import Image\n",
        "from pickle import dump\n",
        "from pickle import load \n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import pickle\n",
        "\n",
        "import tensorflow \n",
        "print(tensorflow.__version__)\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense \n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.backend import categorical_crossentropy\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# setting the random seeds\n",
        "SEED = 4\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "#os.environ['TF_CUDNN_DETERMINISTIC'] = '4'  # new flag present in tf 2.0+\n",
        "np.random.seed(SEED)\n",
        "tensorflow.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvWMoI2AUjvq"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDfkWlh7UR_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "cf81bb9c-a14d-44a6-c3d5-cf3e2940cb16"
      },
      "source": [
        "# read data from the pickle file\n",
        "data = pd.read_pickle(drive_path + '/data_final.pkl')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2610, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR3691</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "      <td>[CXR3691_IM-1842-1001, CXR3691_IM-1842-3003]</td>\n",
              "      <td>(tf.Tensor(0.00026685063, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(4.9858256e-05, shape=(), dtype=floa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR3682</td>\n",
              "      <td>&lt;start&gt; the lungs are hypoventilated there is ...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary abnormality &lt;...</td>\n",
              "      <td>[CXR3682_IM-1834-1001, CXR3682_IM-1834-2001]</td>\n",
              "      <td>(tf.Tensor(0.00033830438, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(6.356468e-05, shape=(), dtype=float...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR3685</td>\n",
              "      <td>&lt;start&gt; calcified thoracic aorta mild rightwar...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary findings &lt;end&gt;</td>\n",
              "      <td>[CXR3685_IM-1836-1001, CXR3685_IM-1836-1002]</td>\n",
              "      <td>(tf.Tensor(0.00016475626, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.0002226108, shape=(), dtype=float...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR37</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "      <td>[CXR37_IM-1847-0001-0001, CXR37_IM-1847-0001-0...</td>\n",
              "      <td>(tf.Tensor(2.0698715e-05, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.00041303012, shape=(), dtype=floa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR3703</td>\n",
              "      <td>&lt;start&gt; the eamination consists of frontal and...</td>\n",
              "      <td>&lt;start&gt; no evidence of acute cardiopulmonary p...</td>\n",
              "      <td>[CXR3703_IM-1850-1001, CXR3703_IM-1850-2001]</td>\n",
              "      <td>(tf.Tensor(0.0003913842, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(5.5506534e-06, shape=(), dtype=floa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                                    IMAGE_FEATURE_2\n",
              "0  CXR3691  ...  (tf.Tensor(4.9858256e-05, shape=(), dtype=floa...\n",
              "1  CXR3682  ...  (tf.Tensor(6.356468e-05, shape=(), dtype=float...\n",
              "2  CXR3685  ...  (tf.Tensor(0.0002226108, shape=(), dtype=float...\n",
              "3    CXR37  ...  (tf.Tensor(0.00041303012, shape=(), dtype=floa...\n",
              "4  CXR3703  ...  (tf.Tensor(5.5506534e-06, shape=(), dtype=floa...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvXCZxwheRQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de9bc3d-0500-4d61-8625-ffa290926f26"
      },
      "source": [
        "# see the unique impressions\n",
        "print(str(len(data.IMPRESSION.unique())) + ' unique impressions')\n",
        "\n",
        "print(data.IMPRESSION.value_counts()[:170])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "661 unique impressions\n",
            "<start> no acute cardiopulmonary abnormality <end>                                                                              388\n",
            "<start> no acute cardiopulmonary findings <end>                                                                                 177\n",
            "<start> no acute cardiopulmonary disease <end>                                                                                  145\n",
            "<start> no acute cardiopulmonary abnormalities <end>                                                                            127\n",
            "<start> no active disease <end>                                                                                                 105\n",
            "                                                                                                                               ... \n",
            "<start> minimal right basilar airspace disease, right middle lobe <end>                                                           1\n",
            "<start> streaky left retrocardiac airspace opacities, in the correct clinical setting this could represent a pneumonia <end>      1\n",
            "<start> mildly limited study with lungs grossly clear <end>                                                                       1\n",
            "<start> no evidence of thoracic injury <end>                                                                                      1\n",
            "<start> slight cardiomegaly with no failure or pneumonia <end>                                                                    1\n",
            "Name: IMPRESSION, Length: 170, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipHtgxMTQAkW"
      },
      "source": [
        "## Compute Word Embeddings using Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGnPujqQWKNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2c5623-d757-42f4-fe13-55b6934509b9"
      },
      "source": [
        "# get all the sentences of the Findings and Impression\n",
        "\n",
        "FIND = data.FINDINGS.values.tolist()\n",
        "IMPR = data.IMPRESSION.values.tolist()\n",
        "\n",
        "# get all the sentences in an array\n",
        "TOTAL = FIND + IMPR\n",
        "\n",
        "# check lengths of the array\n",
        "print(len(FIND))\n",
        "print(len(IMPR))\n",
        "print(len(TOTAL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2610\n",
            "2610\n",
            "5220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Ly-P1gWgUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf82873d-d2e7-49d8-fcab-d0cbdd7e49b3"
      },
      "source": [
        "# create a tokenizer for the total text\n",
        "total_tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "total_tokenizer.fit_on_texts(TOTAL)\n",
        "\n",
        "# get the dict for the total texts\n",
        "total_dict = total_tokenizer.word_index\n",
        "\n",
        "# get the tokens\n",
        "total_tokens = total_tokenizer.texts_to_sequences(TOTAL)\n",
        "\n",
        "# check lengths of the tokens\n",
        "print(len(total_tokens))\n",
        "print(len(total_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5220\n",
            "1314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0MCpfvxcuht"
      },
      "source": [
        "# convert the sentences into array of words\n",
        "\n",
        "def convert_to_sentences(total_tokens):\n",
        "    sentences = list()\n",
        "    # get tokens for a sentence\n",
        "    for tokens in total_tokens:\n",
        "        sent = list()\n",
        "        # add each word to a sent\n",
        "        for token in tokens:\n",
        "            sent.append(total_tokenizer.index_word[token])\n",
        "        sentences.append(sent)\n",
        "    # return the sentences\n",
        "    return sentences\n",
        "\n",
        "total_sentences = convert_to_sentences(total_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY0xNpssQHJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7715f118-ac17-4566-da62-5d3a996eeadc"
      },
      "source": [
        "embedding_dim = 100\n",
        "# define training data\n",
        "sentences = total_sentences\n",
        "# train model\n",
        "w2v_model = Word2Vec(sentences, size=embedding_dim, min_count=1)\n",
        "# print vocab length\n",
        "print(len(w2v_model.wv.vocab))\n",
        "# get all the words\n",
        "total_words = total_tokenizer.word_index.keys()\n",
        "print(len(total_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1314\n",
            "1314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCqm8QusFNlO"
      },
      "source": [
        "# create a dict for the word and embeddings\n",
        "vectors_dict = dict()\n",
        "# total_words is the list of words \n",
        "for key in total_words:\n",
        "    # vectors_dict is the dict of word and embeddings\n",
        "    vectors_dict[key] = w2v_model[key]\n",
        "\n",
        "#print(vectors_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCOc2G6SDxF9"
      },
      "source": [
        "## Split data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYAmxnbsXdLZ"
      },
      "source": [
        "# get the y col and drop the col\n",
        "Y_Data = data.IMPRESSION\n",
        "data.drop('IMPRESSION',axis = 1, inplace = True)\n",
        "\n",
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(data, Y_Data, test_size=0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdvm35NWctCK"
      },
      "source": [
        "data['IMPRESSION'] = Y_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbE2tuLmdNZ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "de398eed-501d-4e72-fcf0-5b8dc58c183e"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>IMPRESSION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR3691</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>[CXR3691_IM-1842-1001, CXR3691_IM-1842-3003]</td>\n",
              "      <td>(tf.Tensor(0.00026685063, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(4.9858256e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR3682</td>\n",
              "      <td>&lt;start&gt; the lungs are hypoventilated there is ...</td>\n",
              "      <td>[CXR3682_IM-1834-1001, CXR3682_IM-1834-2001]</td>\n",
              "      <td>(tf.Tensor(0.00033830438, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(6.356468e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary abnormality &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR3685</td>\n",
              "      <td>&lt;start&gt; calcified thoracic aorta mild rightwar...</td>\n",
              "      <td>[CXR3685_IM-1836-1001, CXR3685_IM-1836-1002]</td>\n",
              "      <td>(tf.Tensor(0.00016475626, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.0002226108, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary findings &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR37</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>[CXR37_IM-1847-0001-0001, CXR37_IM-1847-0001-0...</td>\n",
              "      <td>(tf.Tensor(2.0698715e-05, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.00041303012, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR3703</td>\n",
              "      <td>&lt;start&gt; the eamination consists of frontal and...</td>\n",
              "      <td>[CXR3703_IM-1850-1001, CXR3703_IM-1850-2001]</td>\n",
              "      <td>(tf.Tensor(0.0003913842, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(5.5506534e-06, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no evidence of acute cardiopulmonary p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                                         IMPRESSION\n",
              "0  CXR3691  ...                     <start> no acute disease <end>\n",
              "1  CXR3682  ...  <start> no acute cardiopulmonary abnormality <...\n",
              "2  CXR3685  ...    <start> no acute cardiopulmonary findings <end>\n",
              "3    CXR37  ...                     <start> no acute disease <end>\n",
              "4  CXR3703  ...  <start> no evidence of acute cardiopulmonary p...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efN8ks_coXWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac356ab-5bd3-4179-a63a-3b80d511aa9f"
      },
      "source": [
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2088,)\n",
            "(522,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UmNVvojgil_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d58719b-96a7-4657-a795-c8393f7e5593"
      },
      "source": [
        "# get the shapes of train cv and test data\n",
        "print(type(X_Train),X_Train.shape,type(Y_Train),Y_Train.shape)\n",
        "# lets convert the Y_Train to a dataframe \n",
        "Y_DTrain = pd.DataFrame(data=Y_Train.to_list(), columns=['IMPRESSION'])\n",
        "Y_Train = Y_DTrain\n",
        "\n",
        "print(type(X_Test),X_Test.shape,type(Y_Test),Y_Test.shape)\n",
        "# lets convert the Y_Test to a dataframe\n",
        "Y_DTest = pd.DataFrame(data=Y_Test.to_list(), columns=['IMPRESSION'])\n",
        "Y_Test = Y_DTest\n",
        "\n",
        "print('checking shapes after converting y to dataframe')\n",
        "# lets again check the shapes\n",
        "print(type(X_Train),X_Train.shape,type(Y_Train),Y_Train.shape)\n",
        "print(type(X_Test),X_Test.shape,type(Y_Test),Y_Test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (2088, 5) <class 'pandas.core.series.Series'> (2088,)\n",
            "<class 'pandas.core.frame.DataFrame'> (522, 5) <class 'pandas.core.series.Series'> (522,)\n",
            "checking shapes after converting y to dataframe\n",
            "<class 'pandas.core.frame.DataFrame'> (2088, 5) <class 'pandas.core.frame.DataFrame'> (2088, 1)\n",
            "<class 'pandas.core.frame.DataFrame'> (522, 5) <class 'pandas.core.frame.DataFrame'> (522, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5x-v0cxerFJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0ae38ae0-611a-4ad3-a9ec-11cbb027705f"
      },
      "source": [
        "# create the train dataframe\n",
        "data_train = pd.DataFrame()\n",
        "data_train['UID'] = X_Train.UID.values.tolist()\n",
        "data_train['IMAGES'] = X_Train.IMAGES.values.tolist()\n",
        "data_train['IMAGE_FEATURE_1'] = X_Train.IMAGE_FEATURE_1.values.tolist()\n",
        "data_train['IMAGE_FEATURE_2'] = X_Train.IMAGE_FEATURE_2.values.tolist()\n",
        "data_train['FINDINGS'] = X_Train.FINDINGS.values.tolist()\n",
        "data_train['IMPRESSION'] = Y_Train.IMPRESSION.values\n",
        "print(data_train.shape)\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2088, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR3755</td>\n",
              "      <td>[CXR3755_IM-1879-1001, CXR3755_IM-1879-3001]</td>\n",
              "      <td>(tf.Tensor(6.0409708e-05, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size upper limits of normal pulm...</td>\n",
              "      <td>&lt;start&gt; no acute changes from prior imaging &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR969</td>\n",
              "      <td>[CXR969_IM-2459-1001, CXR969_IM-2459-2001]</td>\n",
              "      <td>(tf.Tensor(8.086289e-05, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size upper limits of normal but ...</td>\n",
              "      <td>&lt;start&gt; no acute radiographic cardiopulmonary ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR771</td>\n",
              "      <td>[CXR771_IM-2316-2001, CXR771_IM-2316-1001]</td>\n",
              "      <td>(tf.Tensor(0.0003665378, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size and pulmonary vascularity w...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary disease &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR371</td>\n",
              "      <td>[CXR371_IM-1852-1001, CXR371_IM-1852-2001]</td>\n",
              "      <td>(tf.Tensor(0.0005545218, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(6.5811844e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; the lungs are clear there is hyperinfl...</td>\n",
              "      <td>&lt;start&gt; copd and old granulomatous disease &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR2645</td>\n",
              "      <td>[CXR2645_IM-1131-2001, CXR2645_IM-1131-1001]</td>\n",
              "      <td>(tf.Tensor(0.00028274013, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(1.4752676e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; surgical clips within the right upper ...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary abnormality &lt;...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                                         IMPRESSION\n",
              "0  CXR3755  ...  <start> no acute changes from prior imaging <end>\n",
              "1   CXR969  ...  <start> no acute radiographic cardiopulmonary ...\n",
              "2   CXR771  ...     <start> no acute cardiopulmonary disease <end>\n",
              "3   CXR371  ...   <start> copd and old granulomatous disease <end>\n",
              "4  CXR2645  ...  <start> no acute cardiopulmonary abnormality <...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im6lKn1YkIKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2ee4022b-5bdc-4b7e-f5a7-c819b628dc05"
      },
      "source": [
        "# create the test dataframe\n",
        "data_test = pd.DataFrame()\n",
        "data_test['UID'] = X_Test.UID.values.tolist()\n",
        "data_test['IMAGES'] = X_Test.IMAGES.values.tolist()\n",
        "data_test['IMAGE_FEATURE_1'] = X_Test.IMAGE_FEATURE_1.values.tolist()\n",
        "data_test['IMAGE_FEATURE_2'] = X_Test.IMAGE_FEATURE_2.values.tolist()\n",
        "data_test['FINDINGS'] = X_Test.FINDINGS.values.tolist()\n",
        "data_test['IMPRESSION'] = Y_Test.IMPRESSION.values\n",
        "print(data_test.shape)\n",
        "data_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(522, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR2327</td>\n",
              "      <td>[CXR2327_IM-0898-1001, CXR2327_IM-0898-2001]</td>\n",
              "      <td>(tf.Tensor(0.0009207953, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(8.276213e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; there has been interval development of...</td>\n",
              "      <td>&lt;start&gt; interval development of large rightsid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR1121</td>\n",
              "      <td>[CXR1121_IM-0080-1001, CXR1121_IM-0080-2001]</td>\n",
              "      <td>(tf.Tensor(0.00041279235, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(8.3250285e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; the lungs are clear there is no pleura...</td>\n",
              "      <td>&lt;start&gt; no acute pulmonary disease &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR40</td>\n",
              "      <td>[CXR40_IM-2050-1001, CXR40_IM-2050-1002]</td>\n",
              "      <td>(tf.Tensor(0.00033369806, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.00030659474, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; mild hyperepansion of the lungs numero...</td>\n",
              "      <td>&lt;start&gt; emphysema with no acute cardiopulmonar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR3263</td>\n",
              "      <td>[CXR3263_IM-1549-1001, CXR3263_IM-1549-2001]</td>\n",
              "      <td>(tf.Tensor(0.00041694037, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(7.052128e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; chest the heart size and cardiomediast...</td>\n",
              "      <td>&lt;start&gt; chest no acute cardiopulmonary finding...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR751</td>\n",
              "      <td>[CXR751_IM-2305-1001, CXR751_IM-2305-2001]</td>\n",
              "      <td>(tf.Tensor(0.000753897, shape=(), dtype=float3...</td>\n",
              "      <td>(tf.Tensor(0.00013644715, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; heart size within normal limits no foc...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary findings &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                                         IMPRESSION\n",
              "0  CXR2327  ...  <start> interval development of large rightsid...\n",
              "1  CXR1121  ...           <start> no acute pulmonary disease <end>\n",
              "2    CXR40  ...  <start> emphysema with no acute cardiopulmonar...\n",
              "3  CXR3263  ...  <start> chest no acute cardiopulmonary finding...\n",
              "4   CXR751  ...    <start> no acute cardiopulmonary findings <end>\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yD73UfVK1yb"
      },
      "source": [
        "## Data Tokenization and Making Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YCgnk26phgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b16c008-f175-4806-d6ed-f16dbd44d9f3"
      },
      "source": [
        "# get the vocab for impressions\n",
        "# impression_tokenizer : tokenizer for impression\n",
        "impression_tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "impression_tokenizer.fit_on_texts(data_train['IMPRESSION'])\n",
        "\n",
        "# saving tokenizer to file\n",
        "with open(drive_path + '/impression_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(impression_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "with open(drive_path + '/impression_tokenizer.pickle', 'rb') as handle:\n",
        "    impression_tokenizer = pickle.load(handle)\n",
        "\n",
        "# get the dict and save it\n",
        "impression_dict = impression_tokenizer.word_index\n",
        "\n",
        "# get the tokens\n",
        "temp_train_impression = impression_tokenizer.texts_to_sequences(data_train['IMPRESSION'])\n",
        "\n",
        "\n",
        "# Now let us get the max length of the findings\n",
        "s = []\n",
        "for d in temp_train_impression:\n",
        "    s.append(len(d))\n",
        "\n",
        "pad_length_impression = max(s)\n",
        "\n",
        "# get the vocab size\n",
        "vocab_size_impression = len(impression_tokenizer.word_index) + 1  \n",
        "\n",
        "# print max lengths\n",
        "print('The vocab size of the impression is',vocab_size_impression)\n",
        "print('The maximum length of the impression is',pad_length_impression)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocab size of the impression is 573\n",
            "The maximum length of the impression is 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enEcFQdj8x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba40f0e-d33a-4d31-9a7a-9355c288bf8a"
      },
      "source": [
        "# get the embedding matrix for the impression\n",
        "# impression_tokenizer is the tokenizer for impression\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "\n",
        "embedding_matrix_impression = zeros((vocab_size_impression, embedding_dim))\n",
        "for word, i in impression_tokenizer.word_index.items():\n",
        "\tembedding_vector = vectors_dict.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix_impression[i] = embedding_vector\n",
        "\n",
        "# saving tokenizer to file\n",
        "with open(drive_path + '/embedding_matrix_impression.pickle', 'wb') as handle:\n",
        "    pickle.dump(embedding_matrix_impression, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "with open(drive_path + '/embedding_matrix_impression.pickle', 'rb') as handle:\n",
        "    embedding_matrix_impression = pickle.load(handle)\n",
        "\n",
        "\n",
        "print(embedding_matrix_impression[1].shape)\n",
        "print(embedding_matrix_impression.shape)\n",
        "\n",
        "impression_matrix = embedding_matrix_impression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100,)\n",
            "(573, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KC7hzbKHsoF"
      },
      "source": [
        "# this function will take the dataframe and return the tokenized and padded findings and impression\n",
        "def tokenize(dataset):\n",
        "    \n",
        "    impression_tensor = impression_tokenizer.texts_to_sequences(dataset.IMPRESSION)\n",
        "\n",
        "    impression_tensor = tensorflow.keras.preprocessing.sequence.pad_sequences(impression_tensor, maxlen = pad_length_impression,\n",
        "                                                         padding='post')\n",
        "\n",
        "    return impression_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1BlVOCmFr1P"
      },
      "source": [
        "# this function will convert the image array into numpy\n",
        "def load_imgs(dataset):\n",
        "    img_feature_1 = dataset.IMAGE_FEATURE_1.values\n",
        "    tmp_arr_1 = np.zeros((len(img_feature_1), 1024))\n",
        "    img_feature_2 = dataset.IMAGE_FEATURE_2.values\n",
        "    tmp_arr_2 = np.zeros((len(img_feature_2), 1024))\n",
        "    #print(tmp_arr_train.shape)\n",
        "    i = 0\n",
        "    for r in img_feature_1:\n",
        "        # print(r)\n",
        "        tmp_arr_1[i] = r\n",
        "        i += 1\n",
        "\n",
        "    img_feature_1 = tmp_arr_1\n",
        "\n",
        "    i = 0\n",
        "    for r in img_feature_2:\n",
        "        # print(r)\n",
        "        tmp_arr_2[i] = r\n",
        "        i += 1\n",
        "\n",
        "    img_feature_2 = tmp_arr_2\n",
        "    \n",
        "    return img_feature_1, img_feature_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXjjcNCGHslj"
      },
      "source": [
        "# this function will load the cleaned images, findings and impression\n",
        "def load_dataset(dataset, purpose = 'testing'):\n",
        "    # creating cleaned input, output pairs\n",
        "    impression_tensor = tokenize(dataset)\n",
        "    img_feature_1, img_feature_2 = load_imgs(dataset)\n",
        "\n",
        "    return img_feature_1, img_feature_2, impression_tensor "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH9-OV4fHsi8"
      },
      "source": [
        "img_feature_1_train, img_feature_2_train, impression_tensor_train = load_dataset(data_train)\n",
        "\n",
        "# print('img_features_train', img_features_train.shape, type(img_features_train), type(img_features_train[0]))\n",
        "# print('findings_tensor_train', findings_tensor_train.shape, type(findings_tensor_train), type(findings_tensor_train[0]))\n",
        "# print('impression_tensor_train', impression_tensor_train.shape, type(impression_tensor_train), type(impression_tensor_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbP6Bxs3-Oco"
      },
      "source": [
        "img_feature_1_test, img_feature_2_test, impression_tensor_test = load_dataset(data_test)\n",
        "\n",
        "# print('img_features_test', img_features_test.shape, type(img_features_test), type(img_features_test[0]))\n",
        "# print('findings_tensor_test', findings_tensor_test.shape, type(findings_tensor_test), type(findings_tensor_test[0]))\n",
        "# print('impression_tensor_test', impression_tensor_test.shape, type(impression_tensor_test), type(impression_tensor_test[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WK1l0Hk3YeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "892e5598-12f7-4ae1-d6fe-730b42753f86"
      },
      "source": [
        "# add the tokens to the dataframe\n",
        "data_impression_tensor = impression_tokenizer.texts_to_sequences(data.IMPRESSION)\n",
        "\n",
        "data['IMPRESSION_TOKENS'] = data_impression_tensor\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>IMPRESSION</th>\n",
              "      <th>IMPRESSION_TOKENS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR3691</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>[CXR3691_IM-1842-1001, CXR3691_IM-1842-3003]</td>\n",
              "      <td>(tf.Tensor(0.00026685063, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(4.9858256e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 6, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR3682</td>\n",
              "      <td>&lt;start&gt; the lungs are hypoventilated there is ...</td>\n",
              "      <td>[CXR3682_IM-1834-1001, CXR3682_IM-1834-2001]</td>\n",
              "      <td>(tf.Tensor(0.00033830438, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(6.356468e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary abnormality &lt;...</td>\n",
              "      <td>[1, 3, 4, 5, 7, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR3685</td>\n",
              "      <td>&lt;start&gt; calcified thoracic aorta mild rightwar...</td>\n",
              "      <td>[CXR3685_IM-1836-1001, CXR3685_IM-1836-1002]</td>\n",
              "      <td>(tf.Tensor(0.00016475626, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.0002226108, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary findings &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 5, 8, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR37</td>\n",
              "      <td>&lt;start&gt; the heart is normal in size the medias...</td>\n",
              "      <td>[CXR37_IM-1847-0001-0001, CXR37_IM-1847-0001-0...</td>\n",
              "      <td>(tf.Tensor(2.0698715e-05, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.00041303012, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no acute disease &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 6, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR3703</td>\n",
              "      <td>&lt;start&gt; the eamination consists of frontal and...</td>\n",
              "      <td>[CXR3703_IM-1850-1001, CXR3703_IM-1850-2001]</td>\n",
              "      <td>(tf.Tensor(0.0003913842, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(5.5506534e-06, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; no evidence of acute cardiopulmonary p...</td>\n",
              "      <td>[1, 3, 13, 9, 4, 5, 11, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...           IMPRESSION_TOKENS\n",
              "0  CXR3691  ...             [1, 3, 4, 6, 2]\n",
              "1  CXR3682  ...          [1, 3, 4, 5, 7, 2]\n",
              "2  CXR3685  ...          [1, 3, 4, 5, 8, 2]\n",
              "3    CXR37  ...             [1, 3, 4, 6, 2]\n",
              "4  CXR3703  ...  [1, 3, 13, 9, 4, 5, 11, 2]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTSS1sA3Yef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "949e636e-a651-45b3-f55f-9925d53760b0"
      },
      "source": [
        "# add the tokens to the dataframe\n",
        "data_train_impression_tensor = impression_tokenizer.texts_to_sequences(data_train.IMPRESSION)\n",
        "\n",
        "data_train['IMPRESSION_TOKENS'] = data_train_impression_tensor\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "      <th>IMPRESSION_TOKENS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR3755</td>\n",
              "      <td>[CXR3755_IM-1879-1001, CXR3755_IM-1879-3001]</td>\n",
              "      <td>(tf.Tensor(6.0409708e-05, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size upper limits of normal pulm...</td>\n",
              "      <td>&lt;start&gt; no acute changes from prior imaging &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 38, 138, 139, 156, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR969</td>\n",
              "      <td>[CXR969_IM-2459-1001, CXR969_IM-2459-2001]</td>\n",
              "      <td>(tf.Tensor(8.086289e-05, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size upper limits of normal but ...</td>\n",
              "      <td>&lt;start&gt; no acute radiographic cardiopulmonary ...</td>\n",
              "      <td>[1, 3, 4, 19, 5, 11, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR771</td>\n",
              "      <td>[CXR771_IM-2316-2001, CXR771_IM-2316-1001]</td>\n",
              "      <td>(tf.Tensor(0.0003665378, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>&lt;start&gt; heart size and pulmonary vascularity w...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary disease &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 5, 6, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR371</td>\n",
              "      <td>[CXR371_IM-1852-1001, CXR371_IM-1852-2001]</td>\n",
              "      <td>(tf.Tensor(0.0005545218, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(6.5811844e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; the lungs are clear there is hyperinfl...</td>\n",
              "      <td>&lt;start&gt; copd and old granulomatous disease &lt;end&gt;</td>\n",
              "      <td>[1, 63, 23, 169, 122, 6, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR2645</td>\n",
              "      <td>[CXR2645_IM-1131-2001, CXR2645_IM-1131-1001]</td>\n",
              "      <td>(tf.Tensor(0.00028274013, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(1.4752676e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; surgical clips within the right upper ...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary abnormality &lt;...</td>\n",
              "      <td>[1, 3, 4, 5, 7, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                IMPRESSION_TOKENS\n",
              "0  CXR3755  ...  [1, 3, 4, 38, 138, 139, 156, 2]\n",
              "1   CXR969  ...          [1, 3, 4, 19, 5, 11, 2]\n",
              "2   CXR771  ...               [1, 3, 4, 5, 6, 2]\n",
              "3   CXR371  ...      [1, 63, 23, 169, 122, 6, 2]\n",
              "4  CXR2645  ...               [1, 3, 4, 5, 7, 2]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqWcRpWQ3Yep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "86366eec-6982-4885-c174-0e1c758e7390"
      },
      "source": [
        "# add the tokens to the dataframe\n",
        "data_test_impression_tensor = impression_tokenizer.texts_to_sequences(data_test.IMPRESSION)\n",
        "\n",
        "data_test['IMPRESSION_TOKENS'] = data_test_impression_tensor\n",
        "\n",
        "data_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>IMAGES</th>\n",
              "      <th>IMAGE_FEATURE_1</th>\n",
              "      <th>IMAGE_FEATURE_2</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "      <th>IMPRESSION_TOKENS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CXR2327</td>\n",
              "      <td>[CXR2327_IM-0898-1001, CXR2327_IM-0898-2001]</td>\n",
              "      <td>(tf.Tensor(0.0009207953, shape=(), dtype=float...</td>\n",
              "      <td>(tf.Tensor(8.276213e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; there has been interval development of...</td>\n",
              "      <td>&lt;start&gt; interval development of large rightsid...</td>\n",
              "      <td>[1, 84, 563, 9, 135, 285, 32, 55, 518, 118, 21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CXR1121</td>\n",
              "      <td>[CXR1121_IM-0080-1001, CXR1121_IM-0080-2001]</td>\n",
              "      <td>(tf.Tensor(0.00041279235, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(8.3250285e-05, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; the lungs are clear there is no pleura...</td>\n",
              "      <td>&lt;start&gt; no acute pulmonary disease &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 12, 6, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CXR40</td>\n",
              "      <td>[CXR40_IM-2050-1001, CXR40_IM-2050-1002]</td>\n",
              "      <td>(tf.Tensor(0.00033369806, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(0.00030659474, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; mild hyperepansion of the lungs numero...</td>\n",
              "      <td>&lt;start&gt; emphysema with no acute cardiopulmonar...</td>\n",
              "      <td>[1, 71, 14, 3, 4, 5, 8, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CXR3263</td>\n",
              "      <td>[CXR3263_IM-1549-1001, CXR3263_IM-1549-2001]</td>\n",
              "      <td>(tf.Tensor(0.00041694037, shape=(), dtype=floa...</td>\n",
              "      <td>(tf.Tensor(7.052128e-05, shape=(), dtype=float...</td>\n",
              "      <td>&lt;start&gt; chest the heart size and cardiomediast...</td>\n",
              "      <td>&lt;start&gt; chest no acute cardiopulmonary finding...</td>\n",
              "      <td>[1, 15, 3, 4, 5, 61, 25, 559, 72, 560, 38, 274...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CXR751</td>\n",
              "      <td>[CXR751_IM-2305-1001, CXR751_IM-2305-2001]</td>\n",
              "      <td>(tf.Tensor(0.000753897, shape=(), dtype=float3...</td>\n",
              "      <td>(tf.Tensor(0.00013644715, shape=(), dtype=floa...</td>\n",
              "      <td>&lt;start&gt; heart size within normal limits no foc...</td>\n",
              "      <td>&lt;start&gt; no acute cardiopulmonary findings &lt;end&gt;</td>\n",
              "      <td>[1, 3, 4, 5, 8, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UID  ...                                  IMPRESSION_TOKENS\n",
              "0  CXR2327  ...  [1, 84, 563, 9, 135, 285, 32, 55, 518, 118, 21...\n",
              "1  CXR1121  ...                                [1, 3, 4, 12, 6, 2]\n",
              "2    CXR40  ...                         [1, 71, 14, 3, 4, 5, 8, 2]\n",
              "3  CXR3263  ...  [1, 15, 3, 4, 5, 61, 25, 559, 72, 560, 38, 274...\n",
              "4   CXR751  ...                                 [1, 3, 4, 5, 8, 2]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZ9xyqLrUVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b5a8b3-3f53-4f6c-83f5-a8a612c9d06a"
      },
      "source": [
        "print('train length ', impression_tensor_train.shape[0])\n",
        "print('test length', impression_tensor_test.shape[0])\n",
        "\n",
        "train_len = impression_tensor_train.shape[0]\n",
        "test_len = impression_tensor_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length  2088\n",
            "test length 522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRv0yaBb_yQm"
      },
      "source": [
        "## Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syRmCCEfOVOu"
      },
      "source": [
        "# setting some vartables\n",
        "BUFFER_SIZE = len(impression_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(impression_tensor_train)//BATCH_SIZE\n",
        "units = 256\n",
        "vocab_tar_size = len(impression_tokenizer.word_index)+1\n",
        "\n",
        "# creating the tensorflow datasets\n",
        "dataset_train = tensorflow.data.Dataset.from_tensor_slices((img_feature_1_train, img_feature_2_train, impression_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_test = tensorflow.data.Dataset.from_tensor_slices((img_feature_1_test, img_feature_2_test, impression_tensor_test)).shuffle(BUFFER_SIZE)\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zfzZge0OVLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a322347d-c16c-486a-9d22-d62a9e2ab84f"
      },
      "source": [
        "# looking at the example batch shapes\n",
        "example_img1_batch, example_img2_batch, example_target_batch = next(iter(dataset_train))\n",
        "example_img1_batch.shape ,example_img2_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 1024]), TensorShape([128, 1024]), TensorShape([128, 40]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N3K8AYZpG3G"
      },
      "source": [
        "### X-Ray Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QIH6Y-rHTF4"
      },
      "source": [
        "# this class is for the x-ray features encoder \n",
        "class Encoder_Xray(tensorflow.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(Encoder_Xray, self).__init__()\n",
        "        # shape after fc == (batch_size, embedding_dim)\n",
        "        self.fc = tensorflow.keras.layers.Dense(embedding_dim, kernel_regularizer=l2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tensorflow.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBH0zteipXyt"
      },
      "source": [
        "### X-Ray Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3veJKZ2Hywa"
      },
      "source": [
        "# this class is for the xray features Attention\n",
        "class BahdanauAttention_Xray(tensorflow.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention_Xray, self).__init__()\n",
        "        self.W1 = tensorflow.keras.layers.Dense(units)\n",
        "        self.W2 = tensorflow.keras.layers.Dense(units)\n",
        "        self.V = tensorflow.keras.layers.Dense(1)\n",
        "        self.add = tensorflow.keras.layers.Add()\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        \n",
        "        # features (Xray_encoder output) shape == (batch_size, embedding_dim)\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "\n",
        "        # features_xray shape == (batch_size, units)\n",
        "        features_xray = self.W1(features)\n",
        "        # features_hidden shape == (batch_size, units)\n",
        "        features_hidden = self.W2(hidden)\n",
        "        # features_add shape == (batch_size, units)\n",
        "        features_add = features_xray + features_hidden\n",
        "        # score shape == (batch_size, units)\n",
        "        score = tensorflow.nn.tanh(features_add)\n",
        "        # features_score shape == (batch_size, 1)\n",
        "        features_score = self.V(score)\n",
        "        # attention_weights shape == (batch_size, 1)\n",
        "        attention_weights = tensorflow.nn.softmax(features_score, axis=1)\n",
        "        # context_vector shape after sum == (batch_size, embedding_dim)\n",
        "        context_vector = attention_weights * features\n",
        "\n",
        "        return context_vector\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkhhSBo8pcL-"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMpY7i3AUrI4"
      },
      "source": [
        "class Decoder(tensorflow.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tensorflow.keras.layers.Embedding(vocab_size, embedding_dim, weights=[impression_matrix], mask_zero=True)\n",
        "        self.gru = tensorflow.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   kernel_regularizer=l2, recurrent_regularizer=l2)\n",
        "        self.fc1 = tensorflow.keras.layers.Dense(self.units, activation='relu', kernel_regularizer=l2)\n",
        "        self.fc2 = tensorflow.keras.layers.Dense(vocab_size, kernel_regularizer=l2)\n",
        "\n",
        "        self.attention1 = BahdanauAttention_Xray(self.units)\n",
        "        self.attention2 = BahdanauAttention_Xray(self.units)\n",
        "\n",
        "\n",
        "    def call(self, x = np.zeros((1,1)), features1 = np.zeros((1,100)), features2 = np.zeros((1,100)), hidden = np.zeros((1,256))):\n",
        "        # defining attention as a separate model\n",
        "        context_vector1 = self.attention1(features1, hidden)\n",
        "        context_vector2 = self.attention2(features2, hidden)\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        x = tensorflow.concat([tensorflow.expand_dims(context_vector1, 1), tensorflow.expand_dims(context_vector2, 1), x], axis=-1)\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim + embedding_dim)\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "        # x_shape == (batch_size * max_length, hidden_size)\n",
        "        x = tensorflow.reshape(x, (-1, x.shape[2]))\n",
        "        # output_shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tensorflow.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbYXA-XLpfPd"
      },
      "source": [
        "### Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTW4tgh4VR3u"
      },
      "source": [
        "# initializing the optimizer and the loss function\n",
        "optimizer = tensorflow.keras.optimizers.Adam(0.01)\n",
        "loss_object = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # get the mask\n",
        "    mask = tensorflow.math.logical_not(tensorflow.math.equal(real, 0))\n",
        "    # calculate loss\n",
        "    loss_ = loss_object(real, pred)\n",
        "    # cast mask\n",
        "    mask = tensorflow.cast(mask, dtype=loss_.dtype)\n",
        "    # loss = loss * mask\n",
        "    loss_ *= mask\n",
        "    # normalize loss\n",
        "    loss_ /= pad_length_impression\n",
        "    # calculate mean and return loss\n",
        "    return tensorflow.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH7_z9e1yRXD"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APR5tS41yOtI"
      },
      "source": [
        "# initialize model\n",
        "encoder1 = Encoder_Xray(embedding_dim)\n",
        "encoder2 = Encoder_Xray(embedding_dim)\n",
        "decoder = Decoder(embedding_dim, units, vocab_tar_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xtdTYPWVRyU"
      },
      "source": [
        "@tensorflow.function\n",
        "def train_step(img_tensor1, img_tensor2, target):\n",
        "    # initialize loss\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the impressions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    # put the first input\n",
        "    dec_input = tensorflow.expand_dims([impression_tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    # using gradient taping\n",
        "    with tensorflow.GradientTape() as tape:\n",
        "        # get image features\n",
        "        features1 = encoder1(img_tensor1)\n",
        "        features2 = encoder2(img_tensor2)\n",
        "        # loop over all the words in the impression\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # get the predections\n",
        "            predictions, hidden = decoder(dec_input, features1, features2, hidden)\n",
        "            # calculate loss\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            # using teacher forcing\n",
        "            dec_input = tensorflow.expand_dims(target[:, i], 1)\n",
        "            #print(dec_input.shape)\n",
        "    total_loss = loss \n",
        "    # get trainable variables\n",
        "    trainable_variables = encoder1.trainable_variables + encoder2.trainable_variables + decoder.trainable_variables\n",
        "    # get gradients\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    # apply gradients\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    # return loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "# function to calculate blue score\n",
        "def calc_blue(img1, img2, target):\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the impressions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    # reshape image features\n",
        "    img1 = tensorflow.keras.backend.reshape(img1, shape=(1, -1))\n",
        "    img2 = tensorflow.keras.backend.reshape(img2, shape=(1, -1))\n",
        "    # get the target sentence\n",
        "    target_sent = list()\n",
        "    for t in target:\n",
        "        target_sent.append(impression_tokenizer.index_word[t])\n",
        "    # get image features\n",
        "    features1 = encoder1(img1)\n",
        "    features2 = encoder2(img2)\n",
        "    # initial decoder input\n",
        "    dec_input = tensorflow.expand_dims([impression_tokenizer.word_index['<start>']], 0)\n",
        "    # initialize the result array\n",
        "    result = []\n",
        "    result.append('<start>')\n",
        "    # loop for the entire pad lenght\n",
        "    for i in range(pad_length_impression):\n",
        "        # predict\n",
        "        predictions, hidden = decoder(dec_input, features1, features2, hidden)\n",
        "\n",
        "        # calculate max\n",
        "        predicted_id = predictions.numpy().argmax()\n",
        "        # append the predicted word to result array\n",
        "        result.append(impression_tokenizer.index_word[predicted_id])\n",
        "        # if '<end>' is reached\n",
        "        if impression_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            # calculate bleu score and return\n",
        "            score = sentence_bleu([target_sent], result)\n",
        "            return score\n",
        "        # the next input to the model is predected at this step\n",
        "        dec_input = tensorflow.expand_dims([predicted_id], 0)\n",
        "        \n",
        "    # calculate score at the end and return it\n",
        "    score = sentence_bleu([target_sent], result)\n",
        "    return score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3d3u7SYVq9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5596f727-6dd9-4cb1-cc93-8674145ffd40"
      },
      "source": [
        "# epochs\n",
        "EPOCHS = 100\n",
        "\n",
        "# define loss\n",
        "training_loss = tensorflow.keras.metrics.Mean(name='training_loss')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    # get the batch\n",
        "    for (batch, (img_tensor1_train, img_tensor2_train, target_train)) in enumerate(dataset_train):\n",
        "        # go to training step\n",
        "        t_loss = train_step(img_tensor1_train, img_tensor2_train, target_train)\n",
        "        # normalize loss over the data\n",
        "        training_loss(t_loss)\n",
        "    \n",
        "    print ('Epoch {} Training Loss {:.6f}'.format(epoch + 1, training_loss.result()))\n",
        "    \n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Training Loss 0.730697\n",
            "Time taken for 1 epoch 34.62364077568054 sec\n",
            "\n",
            "Epoch 2 Training Loss 0.661634\n",
            "Time taken for 1 epoch 1.1008024215698242 sec\n",
            "\n",
            "Epoch 3 Training Loss 0.608223\n",
            "Time taken for 1 epoch 1.1251635551452637 sec\n",
            "\n",
            "Epoch 4 Training Loss 0.565231\n",
            "Time taken for 1 epoch 1.1176791191101074 sec\n",
            "\n",
            "Epoch 5 Training Loss 0.533280\n",
            "Time taken for 1 epoch 1.1171770095825195 sec\n",
            "\n",
            "Epoch 6 Training Loss 0.506912\n",
            "Time taken for 1 epoch 1.1563889980316162 sec\n",
            "\n",
            "Epoch 7 Training Loss 0.484939\n",
            "Time taken for 1 epoch 1.1012299060821533 sec\n",
            "\n",
            "Epoch 8 Training Loss 0.466396\n",
            "Time taken for 1 epoch 1.107560396194458 sec\n",
            "\n",
            "Epoch 9 Training Loss 0.450374\n",
            "Time taken for 1 epoch 1.1234674453735352 sec\n",
            "\n",
            "Epoch 10 Training Loss 0.436693\n",
            "Time taken for 1 epoch 1.1348001956939697 sec\n",
            "\n",
            "Epoch 11 Training Loss 0.424606\n",
            "Time taken for 1 epoch 1.1151831150054932 sec\n",
            "\n",
            "Epoch 12 Training Loss 0.413753\n",
            "Time taken for 1 epoch 1.1401851177215576 sec\n",
            "\n",
            "Epoch 13 Training Loss 0.403740\n",
            "Time taken for 1 epoch 1.1100733280181885 sec\n",
            "\n",
            "Epoch 14 Training Loss 0.394680\n",
            "Time taken for 1 epoch 1.1500706672668457 sec\n",
            "\n",
            "Epoch 15 Training Loss 0.386551\n",
            "Time taken for 1 epoch 1.1642296314239502 sec\n",
            "\n",
            "Epoch 16 Training Loss 0.379024\n",
            "Time taken for 1 epoch 1.1930875778198242 sec\n",
            "\n",
            "Epoch 17 Training Loss 0.371976\n",
            "Time taken for 1 epoch 1.2310028076171875 sec\n",
            "\n",
            "Epoch 18 Training Loss 0.365362\n",
            "Time taken for 1 epoch 1.1635217666625977 sec\n",
            "\n",
            "Epoch 19 Training Loss 0.359420\n",
            "Time taken for 1 epoch 1.1761019229888916 sec\n",
            "\n",
            "Epoch 20 Training Loss 0.353713\n",
            "Time taken for 1 epoch 1.1756048202514648 sec\n",
            "\n",
            "Epoch 21 Training Loss 0.348363\n",
            "Time taken for 1 epoch 1.1936619281768799 sec\n",
            "\n",
            "Epoch 22 Training Loss 0.343371\n",
            "Time taken for 1 epoch 1.1488523483276367 sec\n",
            "\n",
            "Epoch 23 Training Loss 0.338650\n",
            "Time taken for 1 epoch 1.1651794910430908 sec\n",
            "\n",
            "Epoch 24 Training Loss 0.334219\n",
            "Time taken for 1 epoch 1.1356594562530518 sec\n",
            "\n",
            "Epoch 25 Training Loss 0.329963\n",
            "Time taken for 1 epoch 1.170755386352539 sec\n",
            "\n",
            "Epoch 26 Training Loss 0.325819\n",
            "Time taken for 1 epoch 1.1827099323272705 sec\n",
            "\n",
            "Epoch 27 Training Loss 0.322011\n",
            "Time taken for 1 epoch 1.1647157669067383 sec\n",
            "\n",
            "Epoch 28 Training Loss 0.318383\n",
            "Time taken for 1 epoch 1.1736383438110352 sec\n",
            "\n",
            "Epoch 29 Training Loss 0.314855\n",
            "Time taken for 1 epoch 1.1675071716308594 sec\n",
            "\n",
            "Epoch 30 Training Loss 0.311423\n",
            "Time taken for 1 epoch 1.1603968143463135 sec\n",
            "\n",
            "Epoch 31 Training Loss 0.308137\n",
            "Time taken for 1 epoch 1.1839838027954102 sec\n",
            "\n",
            "Epoch 32 Training Loss 0.305053\n",
            "Time taken for 1 epoch 1.1875929832458496 sec\n",
            "\n",
            "Epoch 33 Training Loss 0.302079\n",
            "Time taken for 1 epoch 1.1587505340576172 sec\n",
            "\n",
            "Epoch 34 Training Loss 0.299171\n",
            "Time taken for 1 epoch 1.1644599437713623 sec\n",
            "\n",
            "Epoch 35 Training Loss 0.296350\n",
            "Time taken for 1 epoch 1.1311492919921875 sec\n",
            "\n",
            "Epoch 36 Training Loss 0.293647\n",
            "Time taken for 1 epoch 1.126049280166626 sec\n",
            "\n",
            "Epoch 37 Training Loss 0.291000\n",
            "Time taken for 1 epoch 1.1187641620635986 sec\n",
            "\n",
            "Epoch 38 Training Loss 0.288508\n",
            "Time taken for 1 epoch 1.1297581195831299 sec\n",
            "\n",
            "Epoch 39 Training Loss 0.286117\n",
            "Time taken for 1 epoch 1.1194424629211426 sec\n",
            "\n",
            "Epoch 40 Training Loss 0.283906\n",
            "Time taken for 1 epoch 1.1094002723693848 sec\n",
            "\n",
            "Epoch 41 Training Loss 0.281648\n",
            "Time taken for 1 epoch 1.1175975799560547 sec\n",
            "\n",
            "Epoch 42 Training Loss 0.279366\n",
            "Time taken for 1 epoch 1.131683111190796 sec\n",
            "\n",
            "Epoch 43 Training Loss 0.277090\n",
            "Time taken for 1 epoch 1.1707241535186768 sec\n",
            "\n",
            "Epoch 44 Training Loss 0.274932\n",
            "Time taken for 1 epoch 1.1454501152038574 sec\n",
            "\n",
            "Epoch 45 Training Loss 0.272833\n",
            "Time taken for 1 epoch 1.1368107795715332 sec\n",
            "\n",
            "Epoch 46 Training Loss 0.270794\n",
            "Time taken for 1 epoch 1.100281000137329 sec\n",
            "\n",
            "Epoch 47 Training Loss 0.268760\n",
            "Time taken for 1 epoch 1.1056437492370605 sec\n",
            "\n",
            "Epoch 48 Training Loss 0.266775\n",
            "Time taken for 1 epoch 1.1353638172149658 sec\n",
            "\n",
            "Epoch 49 Training Loss 0.264777\n",
            "Time taken for 1 epoch 1.1065690517425537 sec\n",
            "\n",
            "Epoch 50 Training Loss 0.262850\n",
            "Time taken for 1 epoch 1.1245756149291992 sec\n",
            "\n",
            "Epoch 51 Training Loss 0.260933\n",
            "Time taken for 1 epoch 1.1184687614440918 sec\n",
            "\n",
            "Epoch 52 Training Loss 0.259053\n",
            "Time taken for 1 epoch 1.139070749282837 sec\n",
            "\n",
            "Epoch 53 Training Loss 0.257195\n",
            "Time taken for 1 epoch 1.1674034595489502 sec\n",
            "\n",
            "Epoch 54 Training Loss 0.255385\n",
            "Time taken for 1 epoch 1.14609956741333 sec\n",
            "\n",
            "Epoch 55 Training Loss 0.253596\n",
            "Time taken for 1 epoch 1.1276817321777344 sec\n",
            "\n",
            "Epoch 56 Training Loss 0.251870\n",
            "Time taken for 1 epoch 1.1157681941986084 sec\n",
            "\n",
            "Epoch 57 Training Loss 0.250226\n",
            "Time taken for 1 epoch 1.108766794204712 sec\n",
            "\n",
            "Epoch 58 Training Loss 0.248554\n",
            "Time taken for 1 epoch 1.1162853240966797 sec\n",
            "\n",
            "Epoch 59 Training Loss 0.246924\n",
            "Time taken for 1 epoch 1.1386373043060303 sec\n",
            "\n",
            "Epoch 60 Training Loss 0.245313\n",
            "Time taken for 1 epoch 1.1119611263275146 sec\n",
            "\n",
            "Epoch 61 Training Loss 0.243774\n",
            "Time taken for 1 epoch 1.1330602169036865 sec\n",
            "\n",
            "Epoch 62 Training Loss 0.242235\n",
            "Time taken for 1 epoch 1.1241118907928467 sec\n",
            "\n",
            "Epoch 63 Training Loss 0.240701\n",
            "Time taken for 1 epoch 1.1582400798797607 sec\n",
            "\n",
            "Epoch 64 Training Loss 0.239147\n",
            "Time taken for 1 epoch 1.1207444667816162 sec\n",
            "\n",
            "Epoch 65 Training Loss 0.237638\n",
            "Time taken for 1 epoch 1.1227891445159912 sec\n",
            "\n",
            "Epoch 66 Training Loss 0.236154\n",
            "Time taken for 1 epoch 1.1138889789581299 sec\n",
            "\n",
            "Epoch 67 Training Loss 0.234651\n",
            "Time taken for 1 epoch 1.1125140190124512 sec\n",
            "\n",
            "Epoch 68 Training Loss 0.233169\n",
            "Time taken for 1 epoch 1.1319873332977295 sec\n",
            "\n",
            "Epoch 69 Training Loss 0.231738\n",
            "Time taken for 1 epoch 1.108546257019043 sec\n",
            "\n",
            "Epoch 70 Training Loss 0.230299\n",
            "Time taken for 1 epoch 1.107206106185913 sec\n",
            "\n",
            "Epoch 71 Training Loss 0.228926\n",
            "Time taken for 1 epoch 1.1406936645507812 sec\n",
            "\n",
            "Epoch 72 Training Loss 0.227625\n",
            "Time taken for 1 epoch 1.1265449523925781 sec\n",
            "\n",
            "Epoch 73 Training Loss 0.226286\n",
            "Time taken for 1 epoch 1.1204006671905518 sec\n",
            "\n",
            "Epoch 74 Training Loss 0.224986\n",
            "Time taken for 1 epoch 1.0976505279541016 sec\n",
            "\n",
            "Epoch 75 Training Loss 0.223741\n",
            "Time taken for 1 epoch 1.0968940258026123 sec\n",
            "\n",
            "Epoch 76 Training Loss 0.222464\n",
            "Time taken for 1 epoch 1.114635705947876 sec\n",
            "\n",
            "Epoch 77 Training Loss 0.221165\n",
            "Time taken for 1 epoch 1.1084043979644775 sec\n",
            "\n",
            "Epoch 78 Training Loss 0.219891\n",
            "Time taken for 1 epoch 1.125340461730957 sec\n",
            "\n",
            "Epoch 79 Training Loss 0.218670\n",
            "Time taken for 1 epoch 1.109999179840088 sec\n",
            "\n",
            "Epoch 80 Training Loss 0.217425\n",
            "Time taken for 1 epoch 1.103468656539917 sec\n",
            "\n",
            "Epoch 81 Training Loss 0.216168\n",
            "Time taken for 1 epoch 1.1565206050872803 sec\n",
            "\n",
            "Epoch 82 Training Loss 0.214917\n",
            "Time taken for 1 epoch 1.1280114650726318 sec\n",
            "\n",
            "Epoch 83 Training Loss 0.213711\n",
            "Time taken for 1 epoch 1.118586778640747 sec\n",
            "\n",
            "Epoch 84 Training Loss 0.212516\n",
            "Time taken for 1 epoch 1.1063547134399414 sec\n",
            "\n",
            "Epoch 85 Training Loss 0.211318\n",
            "Time taken for 1 epoch 1.1374905109405518 sec\n",
            "\n",
            "Epoch 86 Training Loss 0.210141\n",
            "Time taken for 1 epoch 1.134995698928833 sec\n",
            "\n",
            "Epoch 87 Training Loss 0.209040\n",
            "Time taken for 1 epoch 1.1064507961273193 sec\n",
            "\n",
            "Epoch 88 Training Loss 0.207985\n",
            "Time taken for 1 epoch 1.1110758781433105 sec\n",
            "\n",
            "Epoch 89 Training Loss 0.206887\n",
            "Time taken for 1 epoch 1.1324565410614014 sec\n",
            "\n",
            "Epoch 90 Training Loss 0.205768\n",
            "Time taken for 1 epoch 1.148775339126587 sec\n",
            "\n",
            "Epoch 91 Training Loss 0.204677\n",
            "Time taken for 1 epoch 1.125121831893921 sec\n",
            "\n",
            "Epoch 92 Training Loss 0.203612\n",
            "Time taken for 1 epoch 1.1406421661376953 sec\n",
            "\n",
            "Epoch 93 Training Loss 0.202520\n",
            "Time taken for 1 epoch 1.1110076904296875 sec\n",
            "\n",
            "Epoch 94 Training Loss 0.201432\n",
            "Time taken for 1 epoch 1.134765386581421 sec\n",
            "\n",
            "Epoch 95 Training Loss 0.200339\n",
            "Time taken for 1 epoch 1.102815866470337 sec\n",
            "\n",
            "Epoch 96 Training Loss 0.199330\n",
            "Time taken for 1 epoch 1.1227707862854004 sec\n",
            "\n",
            "Epoch 97 Training Loss 0.198302\n",
            "Time taken for 1 epoch 1.134676218032837 sec\n",
            "\n",
            "Epoch 98 Training Loss 0.197301\n",
            "Time taken for 1 epoch 1.1548197269439697 sec\n",
            "\n",
            "Epoch 99 Training Loss 0.196289\n",
            "Time taken for 1 epoch 1.1433908939361572 sec\n",
            "\n",
            "Epoch 100 Training Loss 0.195270\n",
            "Time taken for 1 epoch 1.1488752365112305 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvgixpsZJ7On",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf55448-4037-4b8a-e92b-0f53b4fc5a76"
      },
      "source": [
        "start = time.time()\n",
        "# calculate blue scores\n",
        "# train bleu score\n",
        "train_bleu = 0\n",
        "# get train df values\n",
        "data_train_vals = data_train.values\n",
        "for val in data_train_vals:\n",
        "    bleu = calc_blue(val[2], val[3], val[-1])\n",
        "    train_bleu += bleu\n",
        "train_bleu /= train_len\n",
        "\n",
        "print ('Train BLEU score {:.6f}'.format(train_bleu))\n",
        "\n",
        "# test bleu score\n",
        "test_bleu = 0\n",
        "# get test df values\n",
        "data_test_vals = data_test.values\n",
        "for val in data_test_vals:\n",
        "    bleu = calc_blue(val[2], val[3], val[-1])\n",
        "    test_bleu += bleu\n",
        "test_bleu /= test_len\n",
        "\n",
        "print ('Test BLEU score {:.6f}'.format(test_bleu))\n",
        "\n",
        "print ('Time taken for calculating BLEU Score is  {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train BLEU score 0.604076\n",
            "Test BLEU score 0.558759\n",
            "Time taken for calculating BLEU Score is  163.00952649116516 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMn2QVAtBMha"
      },
      "source": [
        "encoder1.save_weights(drive_path + '/encoder1_weights', save_format='tf')\n",
        "encoder2.save_weights(drive_path + '/encoder2_weights', save_format='tf')\n",
        "decoder.save_weights(drive_path + '/decoder_weights', save_format='tf')\n",
        "\n",
        "encoder1.save_weights(drive_path + '/encoder1.h5')\n",
        "encoder2.save_weights(drive_path + '/encoder2.h5')\n",
        "decoder.save_weights(drive_path + '/decoder.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0xpBYcXyYp8"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gifbJhdkZRc"
      },
      "source": [
        "# this function will do greedy search\n",
        "def greedy_search(img1, img2, target):\n",
        "    # reset hidden states\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    # reshape img vectors\n",
        "    img1 = tensorflow.keras.backend.reshape(img1, shape=(1, -1))\n",
        "    img2 = tensorflow.keras.backend.reshape(img2, shape=(1, -1))\n",
        "    # get image features\n",
        "    features1 = encoder1(img1)\n",
        "    features2 = encoder2(img2)\n",
        "    # decoder input = start\n",
        "    dec_input = tensorflow.expand_dims([impression_tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "    result.append('<start>')\n",
        "    # loop for pad length\n",
        "    for i in range(pad_length_impression):\n",
        "        \n",
        "        # get predections\n",
        "        predictions, hidden = decoder(dec_input, features1, features2, hidden)\n",
        "        # get argmax of predicted id\n",
        "        predicted_id = predictions.numpy().argmax()\n",
        "        result.append(impression_tokenizer.index_word[predicted_id])\n",
        "        # if end is reached return\n",
        "        if impression_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result\n",
        "        # predicted output = next input\n",
        "        dec_input = tensorflow.expand_dims([predicted_id], 0)\n",
        "    # return\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VH9mChUWdm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84aec2e3-b436-425c-f2db-a63d0a4d11dd"
      },
      "source": [
        "# get 5 random samples\n",
        "data_eval = data.sample(5)\n",
        "data_eval_vals = data_eval.values\n",
        "\n",
        "# print greedy search outputs\n",
        "for val in data_eval_vals:\n",
        "    print('___________________________________________________NEW__DATA__POINT___________________________________________________')\n",
        "    result = greedy_search(val[3], val[4], val[5])\n",
        "    print('Actual Impression ', val[5])\n",
        "    print('Generated Impression ', result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute cardiopulmonary disease <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'disease', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute cardiopulmonary abnormality <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'abnormality', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute cardiopulmonary disease <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'disease', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute changes from prior imaging <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'abnormality', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> right upper lobe mass, suspicious for neoplasm ct of chest abdomen and head would be helpful for further evaluation <end>\n",
            "Generated Impression  ['<start>', 'right', 'upper', 'lobe', 'mass', 'suspicious', 'for', 'further', 'evaluation', '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-4qNABeWexa"
      },
      "source": [
        "# this function will do beam search\n",
        "def beam_search(img1, img2, target, beam_width = 5):\n",
        "\n",
        "    # reset hidden states\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    # reshape img vectors\n",
        "    img1 = tensorflow.keras.backend.reshape(img1, shape=(1, -1))\n",
        "    img2 = tensorflow.keras.backend.reshape(img2, shape=(1, -1))\n",
        "    # get image features\n",
        "    features1 = encoder1(img1)\n",
        "    features2 = encoder2(img2)\n",
        "    # decoder input = start\n",
        "    start = [impression_tokenizer.word_index['<start>']]\n",
        "    start_word = [[start, 0.0]]\n",
        "    \n",
        "    while len(start_word[0][0]) < pad_length_impression:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            \n",
        "            dec_input = pad_sequences([[s[0][-1]]])\n",
        "            # get the predections \n",
        "            preds, hidden = decoder(dec_input, features1, features2, hidden)\n",
        "            \n",
        "            # Getting the top <beam_width>(n) predictions\n",
        "            top_words = np.argsort(preds).flatten()\n",
        "            word_preds = top_words[-beam_width:]\n",
        "            \n",
        "            # creating a new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_width:]\n",
        "    \n",
        "    # update start word\n",
        "    start_word = start_word[-1][0]\n",
        "    # intermediate caption\n",
        "    intermediate_caption = [impression_tokenizer.index_word[i] for i in start_word]\n",
        "    # generate final captions\n",
        "    final_caption = []\n",
        "    for i in intermediate_caption:\n",
        "        if i != '<end>':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "    # return final captions\n",
        "    final_caption = final_caption[1:]\n",
        "    final_caption.insert(0, '<start>')\n",
        "    if len(final_caption) <= 39:\n",
        "        final_caption.append('<end>')\n",
        "    return final_caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEFb4fISWeuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1602f554-67fe-4c18-b502-8ae120c03a0c"
      },
      "source": [
        "# get 5 random samples\n",
        "data_eval = data.sample(5)\n",
        "data_eval_vals = data_eval.values\n",
        "\n",
        "# print beam search outputs\n",
        "for val in data_eval_vals:\n",
        "    print('___________________________________________________NEW__DATA__POINT___________________________________________________')\n",
        "    result = beam_search(val[3], val[4], val[5], 5)\n",
        "    print('Actual Impression ', val[5])\n",
        "    print('Generated Impression ', result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute cardiopulmonary abnormality <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'abnormality', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> interstitial pulmonary edema <end>\n",
            "Generated Impression  ['<start>', 'cardiomegaly', 'without', 'heart', 'failure', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> large right pneumothora with associated complete collapse of the right lower lobe <end>\n",
            "Generated Impression  ['<start>', 'large', 'right', 'pneumothora', 'with', 'associated', 'complete', 'collapse', 'of', 'the', 'left', 'atrial', 'appendage', 'as', 'aneurysm', 'or', 'atelectasis', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> no acute cardiopulmonary abnormality <end>\n",
            "Generated Impression  ['<start>', 'no', 'acute', 'cardiopulmonary', 'abnormality', '<end>']\n",
            "___________________________________________________NEW__DATA__POINT___________________________________________________\n",
            "Actual Impression  <start> patchy right middle lobe and lingular airspace disease compatible with multilobar pneumonia <end>\n",
            "Generated Impression  ['<start>', 'patchy', 'right', 'middle', 'lobe', 'and', 'lingular', 'airspace', 'disease', '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTOwehZIJQUt"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I63HZgeuJywd"
      },
      "source": [
        "- In this Notebook we used the Bahdanau Attention and Teacher Forcing Technique to train the Models\n",
        "- We used the GRU's as \n",
        "- We used Greedy Search and Beam Search to generate the Impression\n",
        "- We also used Masked Loss in this notebook\n",
        "- The Bleu Score that we got from our model is 0.56"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcweE2DgaFCE"
      },
      "source": [
        "## References\n",
        "\n",
        "- https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
        "- https://www.tensorflow.org/tutorials/text/nmt_with_attention#write_the_encoder_and_decoder_model\n",
        "- https://stackoverflow.com/questions/50786987/multiclass-classification-to-balance-in-python-over-sampling\n",
        "- https://radimrehurek.com/gensim/models/word2vec.html\n",
        "- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "- https://github.com/nagapavan525/radiology-report-generation/blob/master/radiology_report_generation_final/Automated_Radiology_Report_Generation_attention%20(1).ipynb\n",
        "- https://github.com/nagapavan525/radiology-report-generation/blob/master/NewIntegrationWithIndication/1_Capstone-Radiology-PreProcessing.ipynb\n",
        "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "- https://stackoverflow.com/questions/2612548/extracting-an-attribute-value-with-beautifulsoup\n",
        "- https://stackoverflow.com/questions/24962673/beautiful-soup-getting-tag-id\n",
        "- https://stackoverflow.com/a/47091490/4084039\n",
        "- https://www.appservgrid.com/psam/Python_Samplifier--python1compute--Python_Program_to_Find_the_Size_(Resolution)_of_a_Image.html\n",
        "- https://www.geeksforgeeks.org/working-images-python/\n",
        "- https://gist.github.com/sebleier/554280\n",
        "- https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer\n",
        "- https://www.geeksforgeeks.org/python-remove-all-digits-from-a-list-of-strings/\n",
        "- https://stackoverflow.com/questions/12851791/removing-numbers-from-string\n",
        "- https://github.com/nagapavan525/radiology-report-generation/blob/master/radiology_report_generation_final/AutomatedRadiologyReportGenerationWithSentenceEmbeddings.ipynb\n",
        "- https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    }
  ]
}